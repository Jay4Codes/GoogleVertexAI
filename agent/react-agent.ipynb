{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI, VertexAI\n",
    "from langchain.chat_models import ChatOpenAI, ChatVertexAI\n",
    "from langchain.tools import Tool\n",
    "\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.agents import LLMSingleActionAgent\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "\n",
    "from langchain import OpenAI, LLMChain\n",
    "from langchain.tools import DuckDuckGoSearchRun, GoogleSearchRun\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "import re\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_key=\"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\",\n",
    ")\n",
    "\n",
    "# llm2 = VertexAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Got unknown type h",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\Coding\\Google Vertex\\GoogleVertexAI\\agent\\react-agent.ipynb Cell 3\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Coding/Google%20Vertex/GoogleVertexAI/agent/react-agent.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m llm(\u001b[39m\"\u001b[39;49m\u001b[39mhello, how are you?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\langchain\\chat_models\\base.py:349\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[1;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[0;32m    343\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    344\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    348\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[1;32m--> 349\u001b[0m     generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[0;32m    350\u001b[0m         [messages], stop\u001b[39m=\u001b[39mstop, callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    351\u001b[0m     )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m    352\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[0;32m    353\u001b[0m         \u001b[39mreturn\u001b[39;00m generation\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\langchain\\chat_models\\base.py:125\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[0;32m    124\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 125\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    126\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[0;32m    127\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[0;32m    128\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[0;32m    129\u001b[0m ]\n\u001b[0;32m    130\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\langchain\\chat_models\\base.py:115\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[0;32m    113\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 115\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    116\u001b[0m                 m,\n\u001b[0;32m    117\u001b[0m                 stop\u001b[39m=\u001b[39mstop,\n\u001b[0;32m    118\u001b[0m                 run_manager\u001b[39m=\u001b[39mrun_managers[i] \u001b[39mif\u001b[39;00m run_managers \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    119\u001b[0m                 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    120\u001b[0m             )\n\u001b[0;32m    121\u001b[0m         )\n\u001b[0;32m    122\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    123\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\langchain\\chat_models\\base.py:262\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    259\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    260\u001b[0m     )\n\u001b[0;32m    261\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[1;32m--> 262\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(\n\u001b[0;32m    263\u001b[0m         messages, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    265\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\langchain\\chat_models\\openai.py:342\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate\u001b[39m(\n\u001b[0;32m    336\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    337\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    341\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatResult:\n\u001b[1;32m--> 342\u001b[0m     message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_message_dicts(messages, stop)\n\u001b[0;32m    343\u001b[0m     params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m    344\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstreaming:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\langchain\\chat_models\\openai.py:382\u001b[0m, in \u001b[0;36mChatOpenAI._create_message_dicts\u001b[1;34m(self, messages, stop)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`stop` found in both the input and default params.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    381\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m stop\n\u001b[1;32m--> 382\u001b[0m message_dicts \u001b[39m=\u001b[39m [_convert_message_to_dict(m) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m messages]\n\u001b[0;32m    383\u001b[0m \u001b[39mreturn\u001b[39;00m message_dicts, params\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\langchain\\chat_models\\openai.py:382\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`stop` found in both the input and default params.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    381\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m stop\n\u001b[1;32m--> 382\u001b[0m message_dicts \u001b[39m=\u001b[39m [_convert_message_to_dict(m) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m messages]\n\u001b[0;32m    383\u001b[0m \u001b[39mreturn\u001b[39;00m message_dicts, params\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\langchain\\chat_models\\openai.py:135\u001b[0m, in \u001b[0;36m_convert_message_to_dict\u001b[1;34m(message)\u001b[0m\n\u001b[0;32m    129\u001b[0m     message_dict \u001b[39m=\u001b[39m {\n\u001b[0;32m    130\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mfunction\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    131\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: message\u001b[39m.\u001b[39mcontent,\n\u001b[0;32m    132\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: message\u001b[39m.\u001b[39mname,\n\u001b[0;32m    133\u001b[0m     }\n\u001b[0;32m    134\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot unknown type \u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    136\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m message\u001b[39m.\u001b[39madditional_kwargs:\n\u001b[0;32m    137\u001b[0m     message_dict[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m message\u001b[39m.\u001b[39madditional_kwargs[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Got unknown type h"
     ]
    }
   ],
   "source": [
    "llm(\"hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ashraf's Beauty Salon\n",
      "Arihant Sheth\n"
     ]
    }
   ],
   "source": [
    "input_text = \"customer_name: Arihant Sheth, salon_name: Ashraf's Beauty Salon\"\n",
    "salon_name = re.search(r\"salon_name: (.+?)$\", input_text)\n",
    "customer_name = re.search(r\"customer_name: (.+?),\", input_text)\n",
    "print(salon_name.group(1))\n",
    "print(customer_name.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 7, 10, 9, 33, 44, 617424)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_appointment(input_text):\n",
    "\n",
    "    business_name = re.search(r\"business_name: (.+?),\", input_text).group(1)\n",
    "    customer_name = re.search(r\"customer_name: (.+?),\", input_text).group(1)\n",
    "    datetime = re.search(r\"datetime: (.+?),\", input_text).group(1)\n",
    "    purpose = re.search(r\"purpose: (.+?)$\", input_text).group(1)\n",
    "\n",
    "    if business_name == \"[business name]\":\n",
    "        return \"Salon Name not provided.\"\n",
    "    if customer_name == \"[customer name]\":\n",
    "        return \"Customer Name not provided.\"\n",
    "    if datetime == \"[datetime]\":\n",
    "        return \"Date and/or Time not provided.\"\n",
    "    if purpose == \"[purpose]\":\n",
    "        return \"Purpose not provided.\"\n",
    "\n",
    "    try:\n",
    "        datetime = dt.datetime.strptime(datetime, \"%Y-%m-%d %H:%M:%S\")\n",
    "    except:\n",
    "        return \"Ask the user to provide both the date and time for the appointment.\"\n",
    "\n",
    "    if datetime < dt.datetime.now():\n",
    "        return \"Datetime provided is in the past. Please provide a future date and time.\"\n",
    "\n",
    "    print(f\"Salon Name: {business_name}\")\n",
    "    print(f\"Customer Name: {customer_name}\")\n",
    "    print(f\"Date and Time: {datetime}\")\n",
    "    print(f\"Purpose: {purpose}\")\n",
    "    # TODO: Perform API Call to book appointment\n",
    "    # TODO: Customer name optional\n",
    "    return f\"{customer_name}'s {purpose} reservation booked with {business_name} at {datetime}\"\n",
    "\n",
    "\n",
    "search = SerpAPIWrapper(params={\n",
    "    \"engine\": \"google\",\n",
    "    \"location\": \"Mumbai, Maharashtra, India\"}, serpapi_api_key=\"bd68df7e7bc5d6f7340464fd1e5a21c2cefe0c96eacdb13aa51554f099c452cb\"\n",
    ")\n",
    "\n",
    "\n",
    "def ask_within_context(input_text):\n",
    "    return \"You can answer this question yourself.\"\n",
    "\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Book appointment\",\n",
    "        func=book_appointment,\n",
    "        description=f\"Useful only when user details are inputted by the user wants to book the appointment. Do not make up any parameter. Current datetime: {dt.datetime.now()}Pass the customer name, business name, datetime (in %Y-%m-%d %H:%M:%S format), and purpose of the appointment in the following format. Format: customer_name: [customer name], business_name: [business name], datetime: [datetime], purpose: [purpose] \\nDate or Time Missing Example: customer_name: Arihant Sheth, business_name: Ashraf's Beauty Salon, datetime: [datetime], purpose: Haircut \\nBoth date and time should be present, else datetime: [datetime].\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        description=\"Search the web for information.\",\n",
    "        func=search.run,\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"No action required\",\n",
    "        description=\"No action required. Use your own knowledge to answer the question.\",\n",
    "        func=ask_within_context,\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_with_history = \"\"\"You are a compassionate, talkative AI Receptionist. You have to help users with their bookings. Ask the user if they require any help. Use gender neutral pronouns unless they mention their pronouns. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin! Remember to answer as a compansionate AI Receptionist when giving your final answer.\n",
    "\n",
    "Previous conversation history:\n",
    "{history}\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a prompt template\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    # The list of tools available\n",
    "    tools: List[Tool]\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        return self.template.format(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_with_history = CustomPromptTemplate(\n",
    "    template=template_with_history,\n",
    "    tools=tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\", \"history\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOutputParser(AgentOutputParser):\n",
    "    \n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        # Check if agent should finish\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # Parse out the action and action input\n",
    "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "        # Return the action and action input\n",
    "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "    \n",
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_chain = LLMChain(llm=llm, prompt=prompt_with_history)\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_with_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [tool.name for tool in tools]\n",
    "\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain, \n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"], \n",
    "    allowed_tools=tool_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory=ConversationBufferWindowMemory(k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    verbose=False, \n",
    "    memory=memory\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there! How can I assist you today?'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"Hi, how are you doing today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are some good Italian restaurants near you: 1. Around D' Globe, 2. Warehouse Kitchen, 3. Crossroads 92, 4. Daffodils 23, 5. Amar Fast Food & Restaurant, 6. Bombay Eatery, 7. Biscotti - The Cake Cafe. Enjoy your meal! Let me know if there's anything else I can help with.\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"List good Italian restaurants near me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I apologize, but I couldn't find complete information about the price range, rating, and address for the Italian restaurants near you. Is there anything else I can assist you with?\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"Can you give me more information about each of these restaurants? I want to know the price range, the rating, and the address.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The exact address of Around D' Globe Italian restaurant is Mahvir Nagar.\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"What is the exact address of the first restaurant?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salon Name: Achija\n",
      "Customer Name: Arihant Sheth\n",
      "Date and Time: 2023-07-10 20:00:00\n",
      "Purpose: Dinner reservation for 2 people\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Your dinner reservation for 2 people at 8pm today at Achija has been confirmed. Enjoy your meal! Let me know if there's anything else I can assist you with.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"My name is Arihant Sheth. I want a dinner reservation for 2 people at 8pm today at Achija.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salon Name: Ashraf's Salon\n",
      "Customer Name: Arihant Sheth\n",
      "Date and Time: 2023-08-25 17:00:00\n",
      "Purpose: Haircut\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Your appointment at Ashraf's Salon on 25th August 2023 at 5pm has been booked.\""
      ]
     },
     "execution_count": 791,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"5pm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I couldn't find any information about your upcoming dinner reservations. Is there anything else I can assist you with?\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"Do I have any dinner reservations coming up?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lablab_vertexai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
